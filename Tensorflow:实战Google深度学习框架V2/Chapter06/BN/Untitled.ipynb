{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./Data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, initial_weights, activation_fn, use_batch_norm):\n",
    "        \"\"\"\n",
    "        Initializes this object, creating a TensorFlow graph using the given parameters.\n",
    "        \n",
    "        :param initial_weights: list of NumPy arrays or Tensors\n",
    "            Initial values for the weights for every layer in the network. We pass these in\n",
    "            so we can create multiple networks with the same starting weights to eliminate\n",
    "            training differences caused by random initialization differences.\n",
    "            The number of items in the list defines the number of layers in the network,\n",
    "            and the shapes of the items in the list define the number of nodes in each layer.\n",
    "            e.g. Passing in 3 matrices of shape (784, 256), (256, 100), and (100, 10) would \n",
    "            create a network with 784 inputs going into a hidden layer with 256 nodes,\n",
    "            followed by a hidden layer with 100 nodes, followed by an output layer with 10 nodes.\n",
    "        :param activation_fn: Callable\n",
    "            The function used for the output of each hidden layer. The network will use the same\n",
    "            activation function on every hidden layer and no activate function on the output layer.\n",
    "            e.g. Pass tf.nn.relu to use ReLU activations on your hidden layers.\n",
    "        :param use_batch_norm: bool\n",
    "            Pass True to create a network that uses batch normalization; False otherwise\n",
    "            Note: this network will not use batch normalization on layers that do not have an\n",
    "            activation function.\n",
    "        \"\"\"\n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.name = \"With Batch Norm\" if use_batch_norm else \"Without Batch Norm\"\n",
    "\n",
    "        # Batch normalization needs to do different calculations during training and inference,\n",
    "        # so we use this placeholder to tell the graph which behavior to use.\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        # This list is just for keeping track of data we want to plot later.\n",
    "        # It doesn't actually have anything to do with neural nets or batch normalization.\n",
    "        self.training_accuracies = []\n",
    "\n",
    "        # Create the network graph, but it will not actually have any real values until after you\n",
    "        # call train or test\n",
    "        self.build_network(initial_weights, activation_fn)\n",
    "    \n",
    "    def build_network(self, initial_weights, activation_fn):\n",
    "        \"\"\"\n",
    "        Build the graph. The graph still needs to be trained via the `train` method.\n",
    "        \n",
    "        :param initial_weights: list of NumPy arrays or Tensors\n",
    "            See __init__ for description. \n",
    "        :param activation_fn: Callable\n",
    "            See __init__ for description. \n",
    "        \"\"\"\n",
    "        self.input_layer = tf.placeholder(tf.float32, [None, initial_weights[0].shape[0]])\n",
    "        layer_in = self.input_layer\n",
    "        for weights in initial_weights[:-1]:\n",
    "            layer_in = self.fully_connected(layer_in, weights, activation_fn)    \n",
    "        self.output_layer = self.fully_connected(layer_in, initial_weights[-1])\n",
    "   \n",
    "    def fully_connected(self, layer_in, initial_weights, activation_fn=None):\n",
    "        \"\"\"\n",
    "        Creates a standard, fully connected layer. Its number of inputs and outputs will be\n",
    "        defined by the shape of `initial_weights`, and its starting weight values will be\n",
    "        taken directly from that same parameter. If `self.use_batch_norm` is True, this\n",
    "        layer will include batch normalization, otherwise it will not. \n",
    "        \n",
    "        :param layer_in: Tensor\n",
    "            The Tensor that feeds into this layer. It's either the input to the network or the output\n",
    "            of a previous layer.\n",
    "        :param initial_weights: NumPy array or Tensor\n",
    "            Initial values for this layer's weights. The shape defines the number of nodes in the layer.\n",
    "            e.g. Passing in 3 matrix of shape (784, 256) would create a layer with 784 inputs and 256 \n",
    "            outputs. \n",
    "        :param activation_fn: Callable or None (default None)\n",
    "            The non-linearity used for the output of the layer. If None, this layer will not include \n",
    "            batch normalization, regardless of the value of `self.use_batch_norm`. \n",
    "            e.g. Pass tf.nn.relu to use ReLU activations on your hidden layers.\n",
    "        \"\"\"\n",
    "        # Since this class supports both options, only use batch normalization when\n",
    "        # requested. However, do not use it on the final layer, which we identify\n",
    "        # by its lack of an activation function.\n",
    "        if self.use_batch_norm and activation_fn:\n",
    "            # Batch normalization uses weights as usual, but does NOT add a bias term. This is because \n",
    "            # its calculations include gamma and beta variables that make the bias term unnecessary.\n",
    "            # (See later in the notebook for more details.)\n",
    "            weights = tf.Variable(initial_weights)\n",
    "            linear_output = tf.matmul(layer_in, weights)\n",
    "\n",
    "            # Apply batch normalization to the linear combination of the inputs and weights\n",
    "            batch_normalized_output = tf.layers.batch_normalization(linear_output, training=self.is_training)\n",
    "\n",
    "            # Now apply the activation function, *after* the normalization.\n",
    "            return activation_fn(batch_normalized_output)\n",
    "        else:\n",
    "            # When not using batch normalization, create a standard layer that multiplies\n",
    "            # the inputs and weights, adds a bias, and optionally passes the result \n",
    "            # through an activation function.  \n",
    "            weights = tf.Variable(initial_weights)\n",
    "            biases = tf.Variable(tf.zeros([initial_weights.shape[-1]]))\n",
    "            linear_output = tf.add(tf.matmul(layer_in, weights), biases)\n",
    "            return linear_output if not activation_fn else activation_fn(linear_output)\n",
    "\n",
    "    def train(self, session, learning_rate, training_batches, batches_per_sample, save_model_as=None):\n",
    "        \"\"\"\n",
    "        Trains the model on the MNIST training dataset.\n",
    "        \n",
    "        :param session: Session\n",
    "            Used to run training graph operations.\n",
    "        :param learning_rate: float\n",
    "            Learning rate used during gradient descent.\n",
    "        :param training_batches: int\n",
    "            Number of batches to train.\n",
    "        :param batches_per_sample: int\n",
    "            How many batches to train before sampling the validation accuracy.\n",
    "        :param save_model_as: string or None (default None)\n",
    "            Name to use if you want to save the trained model.\n",
    "        \"\"\"\n",
    "        # This placeholder will store the target labels for each mini batch\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=self.output_layer))\n",
    "        \n",
    "        # Define operations for testing\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if self.use_batch_norm:\n",
    "            # If we don't include the update ops as dependencies on the train step, the \n",
    "            # tf.layers.batch_normalization layers won't update their population statistics,\n",
    "            # which will cause the model to fail at inference time\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        else:\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        \n",
    "        # Train for the appropriate number of batches. (tqdm is only for a nice timing display)\n",
    "        for i in tqdm.tqdm(range(training_batches)):\n",
    "            # We use batches of 60 just because the original paper did. You can use any size batch you like.\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(60)\n",
    "            session.run(train_step, feed_dict={self.input_layer: batch_xs, \n",
    "                                               labels: batch_ys, \n",
    "                                               self.is_training: True})\n",
    "        \n",
    "            # Periodically test accuracy against the 5k validation images and store it for plotting later.\n",
    "            if i % batches_per_sample == 0:\n",
    "                test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                                 labels: mnist.validation.labels,\n",
    "                                                                 self.is_training: False})\n",
    "                self.training_accuracies.append(test_accuracy)\n",
    "\n",
    "        # After training, report accuracy against test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                         labels: mnist.validation.labels,\n",
    "                                                         self.is_training: False})\n",
    "        print('{}: After training, final accuracy on validation set = {}'.format(self.name, test_accuracy))\n",
    "\n",
    "        # If you want to use this model later for inference instead of having to retrain it,\n",
    "        # just construct it with the same parameters and then pass this file to the 'test' function\n",
    "        if save_model_as:\n",
    "            tf.train.Saver().save(session, save_model_as)\n",
    "\n",
    "    def test(self, session, test_training_accuracy=False, include_individual_predictions=False, restore_from=None):\n",
    "        \"\"\"\n",
    "        Tests a trained model on the MNIST testing dataset.\n",
    "\n",
    "        :param session: Session\n",
    "            Used to run the testing graph operations.\n",
    "        :param test_training_accuracy: bool (default False)\n",
    "            If True, perform inference with batch normalization using batch mean and variance;\n",
    "            if False, perform inference with batch normalization using estimated population mean and variance.\n",
    "            Note: in real life, *always* perform inference using the population mean and variance.\n",
    "                  This parameter exists just to support demonstrating what happens if you don't.\n",
    "        :param include_individual_predictions: bool (default False)\n",
    "            This function always performs an accuracy test against the entire test set. But if this parameter\n",
    "            is True, it performs an extra test, doing 200 predictions one at a time, and displays the results\n",
    "            and accuracy.\n",
    "        :param restore_from: string or None (default None)\n",
    "            Name of a saved model if you want to test with previously saved weights.\n",
    "        \"\"\"\n",
    "        # This placeholder will store the true labels for each mini batch\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Define operations for testing\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # If provided, restore from a previously saved model\n",
    "        if restore_from:\n",
    "            tf.train.Saver().restore(session, restore_from)\n",
    "\n",
    "        # Test against all of the MNIST test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.test.images,\n",
    "                                                         labels: mnist.test.labels,\n",
    "                                                         self.is_training: test_training_accuracy})\n",
    "        print('-'*75)\n",
    "        print('{}: Accuracy on full test set = {}'.format(self.name, test_accuracy))\n",
    "\n",
    "        # If requested, perform tests predicting individual values rather than batches\n",
    "        if include_individual_predictions:\n",
    "            predictions = []\n",
    "            correct = 0\n",
    "\n",
    "            # Do 200 predictions, 1 at a time\n",
    "            for i in range(200):\n",
    "                # This is a normal prediction using an individual test case. However, notice\n",
    "                # we pass `test_training_accuracy` to `feed_dict` as the value for `self.is_training`.\n",
    "                # Remember that will tell it whether it should use the batch mean & variance or\n",
    "                # the population estimates that were calucated while training the model.\n",
    "                pred, corr = session.run([tf.arg_max(self.output_layer,1), accuracy],\n",
    "                                         feed_dict={self.input_layer: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    self.is_training: test_training_accuracy})\n",
    "                correct += corr\n",
    "\n",
    "                predictions.append(pred[0])\n",
    "\n",
    "            print(\"200 Predictions:\", predictions)\n",
    "            print(\"Accuracy on 200 samples:\", correct/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_accuracies(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Displays a plot of the accuracies calculated during training to demonstrate\n",
    "    how many iterations it took for the model(s) to converge.\n",
    "    \n",
    "    :param args: One or more NeuralNet objects\n",
    "        You can supply any number of NeuralNet objects as unnamed arguments \n",
    "        and this will display their training accuracies. Be sure to call `train` \n",
    "        the NeuralNets before calling this function.\n",
    "    :param kwargs: \n",
    "        You can supply any named parameters here, but `batches_per_sample` is the only\n",
    "        one we look for. It should match the `batches_per_sample` value you passed\n",
    "        to the `train` function.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    batches_per_sample = kwargs['batches_per_sample']\n",
    "    \n",
    "    for nn in args:\n",
    "        ax.plot(range(0,len(nn.training_accuracies)*batches_per_sample,batches_per_sample),\n",
    "                nn.training_accuracies, label=nn.name)\n",
    "    ax.set_xlabel('Training steps')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Validation Accuracy During Training')\n",
    "    ax.legend(loc=4)\n",
    "    ax.set_ylim([0,1])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_and_test(use_bad_weights, learning_rate, activation_fn, training_batches=50000, batches_per_sample=500):\n",
    "    \"\"\"\n",
    "    Creates two networks, one with and one without batch normalization, then trains them\n",
    "    with identical starting weights, layers, batches, etc. Finally tests and plots their accuracies.\n",
    "    \n",
    "    :param use_bad_weights: bool\n",
    "        If True, initialize the weights of both networks to wildly inappropriate weights;\n",
    "        if False, use reasonable starting weights.\n",
    "    :param learning_rate: float\n",
    "        Learning rate used during gradient descent.\n",
    "    :param activation_fn: Callable\n",
    "        The function used for the output of each hidden layer. The network will use the same\n",
    "        activation function on every hidden layer and no activate function on the output layer.\n",
    "        e.g. Pass tf.nn.relu to use ReLU activations on your hidden layers.\n",
    "    :param training_batches: (default 50000)\n",
    "        Number of batches to train.\n",
    "    :param batches_per_sample: (default 500)\n",
    "        How many batches to train before sampling the validation accuracy.\n",
    "    \"\"\"\n",
    "    # Use identical starting weights for each network to eliminate differences in\n",
    "    # weight initialization as a cause for differences seen in training performance\n",
    "    #\n",
    "    # Note: The networks will use these weights to define the number of and shapes of\n",
    "    #       its layers. The original batch normalization paper used 3 hidden layers\n",
    "    #       with 100 nodes in each, followed by a 10 node output layer. These values\n",
    "    #       build such a network, but feel free to experiment with different choices.\n",
    "    #       However, the input size should always be 784 and the final output should be 10.\n",
    "    if use_bad_weights:\n",
    "        # These weights should be horrible because they have such a large standard deviation\n",
    "        weights = [np.random.normal(size=(784,100), scale=5.0).astype(np.float32),\n",
    "                   np.random.normal(size=(100,100), scale=5.0).astype(np.float32),\n",
    "                   np.random.normal(size=(100,100), scale=5.0).astype(np.float32),\n",
    "                   np.random.normal(size=(100,10), scale=5.0).astype(np.float32)\n",
    "                  ]\n",
    "    else:\n",
    "        # These weights should be good because they have such a small standard deviation\n",
    "        weights = [np.random.normal(size=(784,100), scale=0.05).astype(np.float32),\n",
    "                   np.random.normal(size=(100,100), scale=0.05).astype(np.float32),\n",
    "                   np.random.normal(size=(100,100), scale=0.05).astype(np.float32),\n",
    "                   np.random.normal(size=(100,10), scale=0.05).astype(np.float32)\n",
    "                  ]\n",
    "\n",
    "    # Just to make sure the TensorFlow's default graph is empty before we start another\n",
    "    # test, because we don't bother using different graphs or scoping and naming \n",
    "    # elements carefully in this sample code.\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # build two versions of same network, 1 without and 1 with batch normalization\n",
    "    nn = NeuralNet(weights, activation_fn, False)\n",
    "    bn = NeuralNet(weights, activation_fn, True)\n",
    "    \n",
    "    # train and test the two models\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        nn.train(sess, learning_rate, training_batches, batches_per_sample)\n",
    "        bn.train(sess, learning_rate, training_batches, batches_per_sample)\n",
    "    \n",
    "        nn.test(sess)\n",
    "        bn.test(sess)\n",
    "    \n",
    "    # Display a graph of how validation accuracies changed during training\n",
    "    # so we can compare how the models trained and when they converged\n",
    "    plot_training_accuracies(nn, bn, batches_per_sample=batches_per_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
