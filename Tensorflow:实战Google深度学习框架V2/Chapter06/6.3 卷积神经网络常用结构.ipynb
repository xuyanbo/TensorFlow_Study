{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "卷积层是卷积神经网络的重要组成部分，这个部分通常被称为`过滤器(filter)`或`内核(kernel)`。这里使用filter的概念。过滤器是将上一层子节点矩阵转化为下一层神经网络上的一个单位节点矩阵。<br/>\n",
    "在一个卷积层中，filter的长和宽大小需要人工指定，一般取值为$2\\times 2$、$3\\times 3$和$5\\times 5$。因为filter的深度与当前神经网络节点矩阵的深度一致，因此filter矩阵的参数只需要指定两个。<br/>\n",
    "filter前向传播过程是通过左侧小矩阵中的节点计算出右侧单位矩阵节点的过程。假设用$w_{x,y,z}^{i}$来表示对于输出单位节点矩阵中的第i个节点，过滤器输入节点$(x,y,z)$的权重，使用$b^i$表示第i个输出节点对应的偏置项参数，那么单位矩阵中的第i个节点的取值$g(i)$为：\n",
    "$$g(i) = f(\\sum_{x=1}^{2}\\sum_{y=1}^{2}\\sum_{z=1}^{3}a_{x,y,z}\\times w_{x,y,z}^{i} + b^i)$$\n",
    "其中$a_{x,y,z}$为filter中节点{x,y,z}的取值，$f$为激活函数。<br/>\n",
    "下图展示卷积层前向传播计算过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://cs231n.github.io/assets/conv-demo/index.html\"                 width=\"700px\" height=\"700px\" frameborder=\"0\"                 scrolling=\"no\"> </iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<iframe src=\"https://cs231n.github.io/assets/conv-demo/index.html\" \\\n",
    "                width=\"700px\" height=\"700px\" frameborder=\"0\" \\\n",
    "                scrolling=\"no\"> </iframe>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出在当前层的矩阵边界填充0(zero-padding)或者改变filter的步长可以改变前向传播结果矩阵的大小。<br/>\n",
    "以下公式给出了同时使用全0填充时结果矩阵的大小：\n",
    "$$\n",
    "out_{lenght} = \\lceil in_{length}/stride_{lenght} \\rceil \\\\\n",
    "out_{width} = \\lceil in_{width}/stride_{width} \\rceil\n",
    "$$\n",
    "其中$out_{lenght}$表示输出层矩阵的长度，它等于输入层矩阵长度除以长度方向上的步长的向上取整值；类似的，$out_{width}$表示输出层矩阵的宽度，等于输入层矩阵宽度除以宽度方向上的步长的向上取整值。如果不是用全0填充，以下公式给出结果矩阵的大小：\n",
    "$$\n",
    "out_{length} = \\lceil (in_{length}-filter_{length}+1) / stride_{length}\\rceil \\\\\n",
    "out_{width} = \\lceil (in_{width}-filter_{width}+1)/stride_{length}\\rceil\n",
    "$$\n",
    "卷积神经网络的一个重要特性是共享参数，即每一个卷积层中使用的filter的参数都是一样的，这样可以大量减少神经网络上的参数。<br/>\n",
    "TensorFlow对卷积神经网络提供了非常好的支持，以下代码实现了卷积神经网络的前向传播算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 获取filter的共享权重参数，创建一个四维矩阵，第一个和第二个参数是\n",
    "# filter的长和宽，第三个参数表示当前层的深度，第四个参数表示filter\n",
    "# 的深度\n",
    "filter_weights = tf.get_variable(\n",
    "    \"weights\", [5, 5, 3, 16],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "# 和卷积上的权重一样，偏置项也是共享的。\n",
    "biases = tf.get_variable(\n",
    "    \"biases\", [16], \n",
    "    initialializer=tf.constant_initializer(stddev=0.1))\n",
    "# tf.nn.conv2d函数实现卷积神经网络的前向传播算法。\n",
    "# input：当前层的节点矩阵，这个矩阵是一个四维矩阵，\n",
    "#        后面三个维度表示节点矩阵，第一个维度表示输入的batch；\n",
    "# strides: 不同维度上的步长，提供一个4维的数组，第一维和第四维\n",
    "#          数字必须要求是1，因为卷积步长只对长和宽有效；\n",
    "# padding：填充方式，SAME添加全0填充，VALID不添加填充；\n",
    "conv = tf.nn.conv2d(\n",
    "    input, filter_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# tf.nn.bias_add函数实现了给每个节点加上偏置项\n",
    "# 这里不能直接使用加法，因为矩阵不同位置上的节点\n",
    "# 都需要加上同样的偏置项\n",
    "bias = tf.nn.bias_add(conv, biases)\n",
    "# 将计算结果通过Relu函数\n",
    "actived_conv = tf.nn.relu(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层\n",
    "池化层可以非常有效地缩小矩阵的尺寸，从而减少最后连接层的参数。使用池化层既可以甲酸速度又可以防止过拟合问题的发生。<br/>\n",
    "池化层的操作也是通过一个filter结构来完成的，不过池化层不是计算节点矩阵的加权和，而是采用最大值或平均值运算。使用最大值操作的池化层称为最大池化层(max pooling)。使用平均值操作的池化层称为平均值池化层(average pooling)。其他池化层在实践中使用比较少。<br/>\n",
    "与卷积层的filter一样，池化层的filter的尺寸、策略、步长以及是否填充0都需要人工手动设置。唯一的区别的，卷积层的filter是横跨整个深度的，而池化层的filter只影响一个深度上的点，所以池化层的filter除了要在长和宽方向移动之外，还要在深度上移动。<br/>\n",
    "TensorFlow实现最大值池化层的前向传播算法：<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn.max_pooling：最大池化层的前向传播\n",
    "# ksize: filter的尺寸；数组的第一维和第四维必须为1，这说明\n",
    "#        filter不能跨不同深度，实际使用最多的尺寸是[1,2,2,1]\n",
    "#        和[1,3,3,1]\n",
    "pooling = tf.nn.max_pooling(actived_conv, ksize=[1, 3, 3, 1], \n",
    "                           strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow还提供了`tf.nn.average_pooling`来实现平均池化层，调用方式与`tf.nn.max_pooling`函数是一致的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
