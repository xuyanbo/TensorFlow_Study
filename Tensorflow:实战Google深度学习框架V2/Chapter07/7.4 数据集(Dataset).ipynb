{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集的基本使用方法\n",
    "在数据集框架中，每一个数据集代表一种数据源：数据源可能来自一个张量，一个TFRecord文件，一个文本文件，或者经过sharding的一系列文件，等等。由于训练数据通常无法全部写入内存，因此从数据集中读取数据需要一个迭代器按顺序进行读取，这与队列的dequeue()操作和Reader的read()操作相似。与队列相似，数据集也是计算图上的一个节点。<br/>\n",
    "从张量创建数据集的示例：<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_data = [1, 2, 3, 4, 5, 6]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(input_data)\n",
    "# 定义一个迭代器用于遍历数据集\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "# get_next()返回代表一个输入数据的张量，类似于队列的dequeue\n",
    "x = iterator.get_next()\n",
    "y = x * x\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(len(input_data)):\n",
    "        print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据集并读取数据的步骤：\n",
    "1. 定义数据集的构造方法。<br/>\n",
    "使用tf.data.Dataset.from_tensor_slices()表明数据集是从一个张量中构建的。如果数据集是从文件中构建的，则调用不同构造方法。<br/>\n",
    "2. 定义遍历器。<br/>\n",
    "3. 使用get_next()方法从遍历器中读取数据张量，作为计算图其他部分的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'~\\xe6\\x99\\xb4\\xe5\\xa4\\xa9~'\n",
      "b'\\xe5\\x91\\xa8\\xe6\\x9d\\xb0\\xe4\\xbc\\xa6'\n",
      "b'\\xe6\\x95\\x85\\xe4\\xba\\x8b\\xe7\\x9a\\x84\\xe5\\xb0\\x8f\\xe9\\xbb\\x84\\xe8\\x8a\\xb1'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_files = ['./Data/1.txt', './Data/2.txt']\n",
    "dataset = tf.data.TextLineDataset(input_files)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "x = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(3):\n",
    "        print(sess.run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面都是使用最简单的make_one_shot_iterator来遍历数据集。在使用make_one_shot_iterator时，数据集的所有参数都是确定的，因此不需要特殊的初始化过程。如果需要使用placeholder来初始化数据集，则需要使用initializable_iterator。以下使用initializable_iterator来动态初始化数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def parser(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record, \n",
    "        features={\n",
    "            'feat1': tf.FixedLenFeature([], tf.int64),\n",
    "            'feat2': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    return features['feat1'], features['feat2']\n",
    "\n",
    "input_files = tf.placeholder(tf.string)\n",
    "dataset = tf.data.TFRecordDataset(input_files)\n",
    "dataset = dataset.map(parser)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "feat1, feat2 = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={input_files:[...]})\n",
    "    while True:\n",
    "        # 使用try except形式来将数据遍历一遍。在动态指定输入数据时，\n",
    "        # 不同数据来源的数据量大小难以预测，这个方法就不需要我们提前\n",
    "        # 知道数据量的精确大小\n",
    "        try:\n",
    "            sess.run([feat1, feat2])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集的高层操作\n",
    "- **map**<br/>\n",
    "表示对数据集中的每条数据调用map参数中指定的方法。对每一条数据处理后，map将处理后的数据包装成一个新的数据集返回。<br/>\n",
    "- **shuffle**<br/>\n",
    "> dataset = dataset.shuffle(buffer_size)  #随机打乱顺序\n",
    "\n",
    "shuffle方法中的参数`buffer_size`等效于tf.train.shuffle_batch的min_after_dequeue参数。shuffle算法在内部使用一个缓冲区保存buffer_size条数据，每读入一条新数据时，就从缓冲区随机选择一条数据输出。缓冲区越大，随机性能越好，但是消耗内存越多。\n",
    "- **batch**<br/>\n",
    "> dataset = dataset.batch(batch_size)   #将数据组合成batch\n",
    "\n",
    "batch_size代表要输出的每个batch由多少条数据组成。\n",
    "- **repeat**<br/>\n",
    "将数据集中的数据复制多份，其中每一份数据成为一个epoch。\n",
    "> dataset = dataset.repeat(N)    #将数据集重复N份\n",
    "\n",
    "如果在repeat前已经进行shuffle操作，则输出的每个epoch中随机shuffle的结果并不相同。\n",
    "- **concatenate**：将两个数据集顺序连接起来。<br/>\n",
    "- **take**：从数据集中读取前N项数据；<br/>\n",
    "- **skip**：在数据集中跳过前N项数据；<br/>\n",
    "- **flap_map**：从多个数据集中轮流读取数据；<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_files = tf.train.match_filenames_once('./Data/train-*')\n",
    "test_files = tf.train.match_filenames_once('./Data/test-*')\n",
    "\n",
    "def parser(record):\n",
    "    features = tf.parse_single_example(\n",
    "        record, \n",
    "        features={\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'channels': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "    \n",
    "    decoded_image = tf.decoded_raw(features['image'], tf.uint8)\n",
    "    decoded_image.set_shape([features['height'], features['width'], \n",
    "                             features['channels']])\n",
    "    label = features['label']\n",
    "    \n",
    "    return decoded_image, label\n",
    "\n",
    "image_size = 299\n",
    "batch_size = 100\n",
    "shuffle_buffer = 10000\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(train_files)\n",
    "dataset = dataset.map(parser)\n",
    "\n",
    "dataset = dataset.map(lambda image, label:(\n",
    "    preprocess_for_train(image, image_size, image_size, None), label))\n",
    "dataset = dataset.shuffle(shuffle_buffer).batch(batch_size)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "dataset = dataset.repeat(NUM_EPOCHS)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "image_batch, label_batch = iterator.get_next()\n",
    "\n",
    "learning_rate = 0.01\n",
    "logit = inference(image_batch)\n",
    "loss = calc_loss(logit, label_batch)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "                .minimize(loss)\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test_files)\n",
    "test_dataset = test_dataset.map(parser).map(\n",
    "    lambda image, label:(\n",
    "        tf.image.resize_images(image, [image_size, image_size]), label))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# 定义测试集迭代器\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "test_image_batch, test_label_batch = test_iterator.get_next()\n",
    "\n",
    "# 定义测试结果为logit值最大的分类\n",
    "test_logit = inference(test_image_batch)\n",
    "predictions = tf.argmax(test_logit, axis=-1, output_type=tf.int32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run((tf.global_variables_initializer(), \n",
    "              tf.local_variables_initializer()))\n",
    "    # 初始化训练数据的迭代器\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(train_step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "    # 初始化测试数据迭代器\n",
    "    sess.run(test_iterator.initializer)\n",
    "    # 获取测试结果\n",
    "    test_result = []\n",
    "    test_label = []\n",
    "    while True:\n",
    "        try:\n",
    "            pred, label = sess.run([predictions, test_label_batch])\n",
    "            test_result.extend(pred)\n",
    "            test_label.extend(label)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "# 计算精准率\n",
    "correct = [float(y == y_) for (y, y_) in zip(test_result, test_label)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(\"Test accuracy is:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
