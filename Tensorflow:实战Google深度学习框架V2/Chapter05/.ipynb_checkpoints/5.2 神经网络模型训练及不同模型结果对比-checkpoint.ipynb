{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow训练神经网络\n",
    "下面给出完整的TensorFlow程序用于解决MNIST数据集手写体数字识别问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Data/train-images-idx3-ubyte.gz\n",
      "Extracting ../Data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accurace                       using average model is 0.1156\n",
      "After 5000 training step(s), validation accurace                       using average model is 0.984\n",
      "After 10000 training step(s), validation accurace                       using average model is 0.9844\n",
      "After 15000 training step(s), validation accurace                       using average model is 0.9852\n",
      "After 20000 training step(s), validation accurace                       using average model is 0.985\n",
      "After 25000 training step(s), validation accurace                       using average model is 0.9856\n",
      "After 30000 training step(s), test accuracy, using average                 model is 0.9832\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 定义输入节点数\n",
    "INPUT_NODE = 784\n",
    "# 定义输出节点数\n",
    "OUTPUT_NODE = 10\n",
    "# 定义隐藏层节点数\n",
    "LAYER1_NODE = 500\n",
    "# 训练数据集batch大小\n",
    "BATCH_SIZE = 100\n",
    "# 初始学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 正则化系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练轮数\n",
    "TRAINING_STEPS = 30000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "# \n",
    "def inference(input_tensor, aver_class, weight1, bias1, weight2, bias2):\n",
    "    if aver_class == None:\n",
    "        # 计算前向传播隐藏层输出结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weight1) + bias1)\n",
    "        # 计算输出层的计算结果。因为在计算损失函数时会一并计算softmax函数，\n",
    "        # 所以这里不要调用激活函数\n",
    "        return tf.matmul(layer1, weight2) + bias2\n",
    "    else:\n",
    "        # 首先计算变量的滑动平均值，在计算前向传播输出结果\n",
    "        layer1 = tf.nn.relu(\n",
    "            tf.matmul(input_tensor, aver_class.average(weight1)) \n",
    "            + aver_class.average(bias1))\n",
    "        return tf.matmul(layer1, aver_class.average(weight2)) \\\n",
    "                + aver_class.average(bias2)\n",
    "# 神经网络训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-output')\n",
    "    # 随机生成权重和偏置项\n",
    "    weight1 = tf.Variable(\n",
    "        tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    bias1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    weight2 = tf.Variable(\n",
    "        tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    bias2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    # 计算前向传播结果\n",
    "    y = inference(x, None, weight1, bias1, weight2, bias2)\n",
    "    # 定义存储训练轮数的变量。无需计算滑动平均值，所以指定该变量为不可训练的变量\n",
    "    global_steps = tf.Variable(0, trainable=False)\n",
    "    # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类\n",
    "    variables_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_steps)\n",
    "    # 对神经网络参数的变量使用滑动平均\n",
    "    variables_averages_op = variables_averages.apply(\n",
    "        tf.trainable_variables())\n",
    "    # 计算使用滑动平均后的前向传播计算结果\n",
    "    average_y = inference(\n",
    "        x, variables_averages, weight1, bias1, weight2, bias2)\n",
    "    # 计算交叉熵的值\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=y, labels=tf.argmax(y_, 1))\n",
    "    # 计算batch所用样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    # 计算L2正则化\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weight1) + regularizer(weight2)\n",
    "    # 总损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    # 设置学习率衰减率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE, \n",
    "        global_steps, \n",
    "        mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "                    .minimize(loss, global_steps)\n",
    "    # 在训练神经网络模型时，每过一遍数据都要通过反向传播来更新神经网络参数\n",
    "    # 又要更新每个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    # 检验前向传播结果是否正确\n",
    "    correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(average_y, 1))\n",
    "    # 数据正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # 验证数据集\n",
    "        validate_feed = {x: mnist.validation.images, \n",
    "                         y_: mnist.validation.labels}\n",
    "        # 测试数据集\n",
    "        test_feed = {x: mnist.test.images,\n",
    "                     y_: mnist.test.labels}\n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 5000 == 0:\n",
    "                validate_acc = sess.run(accuracy, validate_feed)\n",
    "                print('After %d training step(s), validation accurace \\\n",
    "                      using average model is %g' % (i, validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x:xs, y_:ys})\n",
    "        test_acc = sess.run(accuracy, test_feed)\n",
    "        print('After %d training step(s), test accuracy, using average \\\n",
    "                model is %g' % (TRAINING_STEPS, test_acc))\n",
    "\n",
    "# def main(argv = None):\n",
    "#     mnist = input_data.read_data_sets('../Data', one_hot=True)\n",
    "#     train(mnist)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "#     tf.app.run()\n",
    "    mnist = input_data.read_data_sets('../Data', one_hot=True)\n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.argmax** ：返回张量轴上具有最大值的索引。<br/>\n",
    "tf.math.argmax(<br/>\n",
    "    input,<br/>\n",
    "    axis=None,<br/>\n",
    "    name=None,<br/>\n",
    "    dimension=None,<br/>\n",
    "    output_type=tf.int64<br/>\n",
    ")<br/>\n",
    "**input**：tensor\n",
    "**axis**：描述输入Tensor的操作的轴向。axis=1对列进行操作，axis=0对行进行操作\n",
    "**output_type**：输出类型\n",
    "**name**：操作名称(可选)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.cast(x, dtype, name)**：将x的数据格式转化成dtype。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.control_dependencies(control_inputs)**：<br/>\n",
    "用于控制计算流图的先后顺序的。该函数接受的参数control_inputs，是Operation或者Tensor构成的list。返回的是一个上下文管理器，该上下文管理器用来控制在该上下文中的操作的依赖。也就是说，上下文管理器下定义的操作是依赖control_inputs中的操作的，control_dependencies用来控制control_inputs中操作执行后，才执行上下文管理器中定义的操作。<br/>\n",
    "但是，tensorflow是顺序执行的，为什么还需control_dependecies呢？原因在实际训练中，大多是以一个BATCH_SIZE大小来训练。因此需循环地去刷新前面所定义的变量。而采用control_dependencies可以确保control_input在被刷新之后,在执行定义的内容，从而保证计算顺序的正确性。<br/>\n",
    "control_dependencies(control_inputs)使用示例：\n",
    "```\n",
    " a、b、c执行后d、e运行\n",
    "with tf.control_dependencies([a,b,c]):\n",
    "    d = ...\n",
    "    e = ...\n",
    "\n",
    " 嵌套使用\n",
    "with tf.control_dependencies([a,b]):\n",
    "    a、b执行后构建下面的control_dependencies\n",
    "    with tf.control_dependencies([c,d]):\n",
    "      a、b、c、d执行后，运行上下文的内容\n",
    "\n",
    " 传入None消除依赖\n",
    "with tf.control_dependencies([a,b]):\n",
    "    在a、b执行后进行构建\n",
    "    with tf.control_dependencies(None):\n",
    "        不用等a、b执行后就可以构建\n",
    "        with tf.control_dependencies([c,d]):\n",
    "            在c、d执行后，不必等a、b执行后构建\n",
    "```\n",
    "控制依赖只对那些在上下文环境中建立的操作有效，仅仅在context中使用一个操作或张量是没用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用验证数据集判断模型效果\n",
    "为了评测神经网络在不同参数小的效果，一般会从训练数据集里面选取一部分数据作为验证数据。使用验证数据可以评判不同参数取值下模型的表现。除了使用验证数据集之外，还可以使用交叉验证(cross validation)来验证模型效果。但是因为神经网络本身训练时间比较长，采用cross validation会花费大量时间。所以在海量数据集的情况下，一般会更多的采用验证数据集的方式来评测模型的效果。<br/>\n",
    "不同问题的数据分布不一样，如果验证数据分布不能很好代表测试数据分布，那么模型在这两种数据集上的效果表现可能会不一样。所以，验证数据选取的方式很重要，一般来说验证数据的选取越接近测试数据的分布，模型在验证数据上的表现越能提现模型在测试数据上的表现。<br/>\n",
    "## 不同模型效果比较\n",
    "- 调整神经网络模型的结构对输出结果有较大影响。<br/>\n",
    "- 使用滑动平均模型和指数衰减的学习率在一定程度上限制了模型参数的更新速度，当模型的收敛速度很快时，这两种优化手段对最终影响不大。<br/>\n",
    "- 使用正则化的损失函数可以给模型效果带来明显提升。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
